{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a25570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "uri = os.getenv(\"MONGODB_URI\")\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri)\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb7f192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from db import db\n",
    "from bson.objectid import ObjectId\n",
    "db = client[\"ResumeParser\"]\n",
    "resumes = db[\"resumes\"]\n",
    "\n",
    "def insert_resume(email, resume_data):\n",
    "    return resumes.insert_one({\"email\": email, \"resume\": resume_data}).inserted_id\n",
    "\n",
    "def get_resumes_by_email(email):\n",
    "    return list(resumes.find({\"email\": email}))\n",
    "\n",
    "def get_resume_by_id(resume_id):\n",
    "    return resumes.find_one({\"_id\": ObjectId(resume_id)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2327f1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectId('68734ea3aa1b389788fec498')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_resume(\"praveentak715@gmail.com\", \"account123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9b6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import bcrypt\n",
    "\n",
    "users = db[\"users\"]\n",
    "\n",
    "def register_user(email, password):\n",
    "    if users.find_one({\"email\": email}):\n",
    "        return False, \"Email already exists\"\n",
    "    hashed = bcrypt.hashpw(password.encode(), bcrypt.gensalt())\n",
    "    users.insert_one({\"email\": email, \"password\": hashed})\n",
    "    return True, \"User registered\"\n",
    "\n",
    "def validate_login(email, password):\n",
    "    user = users.find_one({\"email\": email})\n",
    "    if user and bcrypt.checkpw(password.encode(), user[\"password\"]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_user_by_email(email):\n",
    "    if users.find_one({\"email\": email}):\n",
    "        return True, \"Email already exists\"\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f96012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'Email already exists')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_user_by_email(\"praveentak715@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb74ca9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 'Email already exists')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "register_user(\"praveentak715@gmail.com\", \"account123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4821f956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_login(\"praveentak715@gmail.com\", \"account123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e168c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcrypt\n",
    "\n",
    "def hash_password(password):\n",
    "    return bcrypt.hashpw(password.encode(), bcrypt.gensalt())\n",
    "\n",
    "def verify_password(password, hashed):\n",
    "    return bcrypt.checkpw(password.encode(), hashed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152bdd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import blankline_tokenize\n",
    "import json\n",
    "\n",
    "class ResumeParser:\n",
    "    def __init__(self, file):\n",
    "        self.SECTION_KEYWORDS = {\n",
    "            \"personal_info\": [\n",
    "                \"Personal Information\", \"Contact Information\", \"Contact Details\", \"Profile\", \"About Me\", \"Bio\"\n",
    "            ],\n",
    "            \"objective\": [\n",
    "                \"Objective\", \"Career Objective\", \"Professional Summary\", \"Summary\", \"Profile Summary\", \"Career Summary\", \"Personal Statement\", \"Executive Summary\"\n",
    "            ],\n",
    "            \"skills\": [\n",
    "                \"Skills\", \"Key Skills\", \"Technical Skills\", \"Core Competencies\", \"Competencies\", \"Areas of Expertise\", \"Technical Proficiencies\"\n",
    "            ],\n",
    "            \"experience\": [\n",
    "                \"Work Experience\", \"Professional Experience\", \"Employment History\", \"Experience\", \"Relevant Experience\", \"Work History\", \"Career History\"\n",
    "            ],\n",
    "            \"education\": [\n",
    "                \"Education\", \"Educational Qualifications\", \"Academic Background\", \"Academic History\", \"Academic Qualifications\", \"Educational Background\"\n",
    "            ],\n",
    "            \"certifications\": [\n",
    "                \"Certifications\", \"Certificates\", \"Professional Certifications\", \"Licenses\", \"Training\"\n",
    "            ],\n",
    "            \"projects\": [\n",
    "                \"Projects\", \"Key Projects\", \"Project Experience\", \"Significant Projects\", \"Relevant Projects\"\n",
    "            ],\n",
    "            \"achievements\": [\n",
    "                \"Achievements\", \"Accomplishments\", \"Key Achievements\", \"Awards and Honors\", \"Honors\", \"Recognitions\"\n",
    "            ],\n",
    "            \"languages\": [\n",
    "                \"Languages\", \"Language Proficiency\", \"Language Skills\"\n",
    "            ],\n",
    "            \"interests\": [\n",
    "                \"Interests\", \"Hobbies\", \"Personal Interests\", \"Extra-Curricular Activities\"\n",
    "            ],\n",
    "            \"references\": [\n",
    "                \"References\", \"Referees\"\n",
    "            ],\n",
    "            \"publications\": [\n",
    "                \"Publications\", \"Research Publications\", \"Papers\", \"Articles\"\n",
    "            ],\n",
    "            \"volunteer\": [\n",
    "                \"Volunteer Experience\", \"Volunteering\", \"Community Involvement\", \"Social Service\"\n",
    "            ],\n",
    "            \"additional\": [\n",
    "                \"Additional Information\", \"Other Information\", \"Miscellaneous\"\n",
    "            ],\n",
    "            \"development\": [\n",
    "                \"Professional Development\", \"Continuing Education\", \"Workshops\", \"Seminars\", \"Conferences\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self.file = file\n",
    "\n",
    "    def data_ingestion(self):\n",
    "        # Check the file extension to determine how to read the file\n",
    "        _, file_extension = os.path.splitext(self.file)\n",
    "\n",
    "        if file_extension == '.txt':\n",
    "            with open(self.file, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "        \n",
    "        elif file_extension == '.docx':\n",
    "            content = docx2txt.process(self.file)\n",
    "        \n",
    "        elif file_extension == '.pdf':\n",
    "            content = \"\"\n",
    "            with open(self.file, \"rb\") as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                for page in reader.pages:\n",
    "                    content += page.extract_text() + \"\\n\"\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format: {}\".format(file_extension))\n",
    "\n",
    "        return content\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess(self):\n",
    "        content = self.data_ingestion()\n",
    "        tokenized = blankline_tokenize(content)\n",
    "        return tokenized\n",
    "\n",
    "    def extract_resume_info(self):\n",
    "       text = self.preprocess()\n",
    "       # Join the tokenized content into a single string\n",
    "       text = \"\\n\".join(text)\n",
    "       lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "       \n",
    "       # ---------- 1. Extract Phone (10-digit, Indian style) ----------\n",
    "       phone = re.findall(r'\\b[6-9]\\d{9}\\b', text)\n",
    "       phone = phone[0] if phone else None\n",
    "\n",
    "       # ---------- 2. Extract Email ----------\n",
    "       email = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', text)\n",
    "       email = email[0] if email else None\n",
    "\n",
    "       # ---------- 3. Extract Links ----------\n",
    "       raw_urls = re.findall(r'https?://[^\\s)>\\]]+', text)\n",
    "       markdown_urls = re.findall(r'\\[.*?\\]\\((https?://[^\\s)]+)\\)', text)\n",
    "       html_urls = re.findall(r'href=\"(https?://[^\\s\"]+)\"', text)\n",
    "       links = list(set(raw_urls + markdown_urls + html_urls))\n",
    "\n",
    "       # ---------- 4. Extract Name ----------\n",
    "       # Assume name is first non-empty line and contains no @ or digits\n",
    "       name = None\n",
    "       for line in lines[:5]:\n",
    "           if not any(char.isdigit() for char in line) and '@' not in line and len(line.split()) <= 5:\n",
    "               name = line\n",
    "               break\n",
    "\n",
    "       return {\n",
    "           \"name\": name,\n",
    "           \"phone\": phone,\n",
    "           \"email\": email,\n",
    "           \"links\": links,\n",
    "       }\n",
    "   \n",
    "\n",
    "    def section_identification(self):\n",
    "        document = self.data_ingestion()\n",
    "        tokenized = self.preprocess()\n",
    "\n",
    "        sections = {}\n",
    "        sections_list = []\n",
    "        sections_index = {}\n",
    "        previous_section = None\n",
    "\n",
    "        resume_info = self.extract_resume_info()\n",
    "        sections.update(resume_info)\n",
    "            \n",
    "        for num in range(len(tokenized)):\n",
    "            section_name = tokenized[num].split(\"\\n\")[0].title()\n",
    "            element = tokenized[num].split(\"\\n\")[0].strip().title()\n",
    "            sections_list.append(element)\n",
    "            for section, keywords in self.SECTION_KEYWORDS.items():\n",
    "                if any(keyword in section_name for keyword in keywords):\n",
    "                    section_name = section_name.strip()\n",
    "                    sections_index[section_name] = sections_list.index(section_name)\n",
    "                    current_section = section\n",
    "                    sections[current_section] = []\n",
    "                    if previous_section:\n",
    "                        sections[ps] = (tokenized[sections_index[previous_section]:sections_index[section_name]])\n",
    "                        previous_section = section_name\n",
    "                        ps = current_section\n",
    "                    else:\n",
    "                        previous_section = section_name\n",
    "                        ps = current_section\n",
    "\n",
    "            for section, details in sections.items():\n",
    "                sections[section] = str(\"\".join(details))   \n",
    "\n",
    "        return sections\n",
    "\n",
    "\n",
    "    def csv_format(self):\n",
    "        json_data = self.section_identification()\n",
    "        \n",
    "        # Convert JSON data to a dictionary if it's a JSON string\n",
    "        if isinstance(json_data, str):\n",
    "            json_data = json.loads(json_data)\n",
    "\n",
    "        # Define the uploads directory\n",
    "        uploads_dir = \"uploads\"\n",
    "        os.makedirs(uploads_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "        # Define the CSV file path\n",
    "        csv_file_path = os.path.join(uploads_dir, \"resume_data.csv\")\n",
    "        \n",
    "        # Open the CSV file for writing\n",
    "        with open(csv_file_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            \n",
    "            # Write the header\n",
    "            writer.writerow([\"Section\", \"Content\"])\n",
    "\n",
    "            # Iterate through the JSON data and write to CSV\n",
    "            for section, content in json_data.items():\n",
    "                if isinstance(content, list):\n",
    "                    for item in content:\n",
    "                        writer.writerow([section, item])\n",
    "                else:\n",
    "                    writer.writerow([section, str(content)])\n",
    "\n",
    "        # Simulate file download (in a real application, you would return the file)\n",
    "        # For example, in a Flask app, you would use send_file(csv_file_path)\n",
    "\n",
    "        # After the file is downloaded, delete it\n",
    "        # os.remove(csv_file_path)\n",
    "\n",
    "\n",
    "    def json_format(self):\n",
    "        raw_text = self.section_identification()\n",
    "        \n",
    "        # Define the uploads directory\n",
    "        uploads_dir = \"uploads\"\n",
    "        os.makedirs(uploads_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "        # Define the JSON file path\n",
    "        json_file_path = os.path.join(uploads_dir, \"resume_data.json\")\n",
    "        \n",
    "        # Write the JSON data to the file\n",
    "        with open(json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(raw_text, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        # Read the JSON data back from the file\n",
    "        with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = json.load(f)\n",
    "\n",
    "        return content\n",
    "\n",
    "\n",
    "    def excel_format(self):\n",
    "        raw_text = self.section_identification()\n",
    "        rows = []\n",
    "        \n",
    "        # Prepare the data for the DataFrame\n",
    "        for section, items in raw_text.items():\n",
    "            if isinstance(items, list):\n",
    "                for item in items:\n",
    "                    rows.append({\"Section\": section, \"Content\": item})\n",
    "            else:\n",
    "                rows.append({\"Section\": section, \"Content\": str(items)})\n",
    "\n",
    "        # Define the uploads directory\n",
    "        uploads_dir = \"uploads\"\n",
    "        os.makedirs(uploads_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "        # Define the Excel file path\n",
    "        excel_file_path = os.path.join(uploads_dir, \"resume_data.xlsx\")\n",
    "        \n",
    "        # Create a DataFrame and save it to an Excel file\n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "        # Read the Excel file back (if needed)\n",
    "        # You can use pd.read_excel() if you want to read it back into a DataFrame\n",
    "        # df_read = pd.read_excel(excel_file_path)\n",
    "\n",
    "        # Delete the Excel file after reading\n",
    "        os.remove(excel_file_path)\n",
    "\n",
    "        return df  # Return the DataFrame if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab27548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_resume(email, resume_data):\n",
    "    inserted = resumes.insert_one({\"email\": email, \"resume\": resume_data})\n",
    "    return \"Inserted ID:\", inserted.inserted_id\n",
    "\n",
    "def get_format_resume(id, email, format):\n",
    "    if format == \"json\":\n",
    "        extracted = resumes.find_one({ \"$and\" : [{ \"id\": id }, { \"email\": email }]}, {\"json\": 1})\n",
    "    elif format == \"csv\":\n",
    "        extracted = resumes.find_one({ \"$and\" : [{ \"id\": id }, { \"email\": email }]}, {\"csv\": 1})\n",
    "    elif format == \"json\":\n",
    "        extracted = resumes.find_one({ \"$and\" : [{ \"id\": id }, { \"email\": email }]}, {\"excel\": 1})\n",
    "    else:\n",
    "        return \"No data matched\"\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38f163b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "rp = ResumeParser(\"Resume.docx\") \n",
    "email = \"praveentak715@gmail.com\"\n",
    "\n",
    "id = \"1245\"\n",
    "fjson = rp.json_format()\n",
    "fcsv = rp.csv_format()\n",
    "fexcel = rp.excel_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd92bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response 0 bytes [200 OK]>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "715b6fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted ID: 687fc588438f205d7e05fc4e\n",
      "687fc588438f205d7e05fc4e,\n",
      "id=1245,\n",
      "email=praveentak715@gmail.com,\n",
      "resume data=Resume.docx,\n",
      "{'name': 'Praveen Tak', 'phone': '9462096002', 'email': 'praveentak715@gmail.com', 'links': '', 'personal_info': 'PROFILE', 'experience': 'WORK EXPERIENCEFeb 2025 – Apr 2025\\tProject Trainee – Defence Research and Development Organisation (DRDO)\\t\\t\\t\\t\\t Jodhpur, India\\nDomain: Machine Learning | Computer Vision | Deep LearningKey Responsibilities & Learnings:Worked on the project “Painting Generation using CycleGANs” exploring advanced concepts in image-to-image translation using Generative Adversarial Networks.Implemented neural network models using Python, TensorFlow/PyTorch, and other ML libraries.Performed data preprocessing, augmentation, and evaluation of generated outputs.', 'projects': 'PROJECTSJan 2025  Feb 2025\\tMushroom Classification — Machine Learning', 'skills': 'Aug 2023  Jan 2025\\tFull Stack Data Science Pro — PW SkillsOct 2024\\tCodemathon 2024 — Department of Mathematics, NIT KurukshetraFeb 2024  Feb 2024\\tPython Stack — Great LearningOct 2023  Oct 2023\\tPython for Data Science — UdemyMay 2023\\tLearn Python & Machine Learning — Microsoft Learn Student Ambassador, Google Developer Student ClubsMar 2023\\tCloud Technical Series — Google', 'interests': 'EXTRA-CURRICULAR ACTIVITIESPoster Making Competition Winner – National Mathematics DayActive participant in coding competitions and hackathonsCalligraphy Competition Winner  APS Session 2021-22National Reading (English)  APS Session 2021-22', 'education': 'EDUCATIONSep 2022 – Jul 2025 \\tBachelor of Science (PMCS), Lachoo Memorial    \\t          84.50% of College Science & Technology, JodhpurMay 2021 – Jul 2022 \\tXII (Science), Army Public School, Jodhpur \\t\\t          85.00%Apr 2019 – Apr 2020 \\tX, Army Public School, Jodhpur\\t\\t                       87.83%', 'languages': 'Programming Languages: Python, Java, SQL, C, C++Machine Learning & Data Analysis: Scikit-learn, TensorFlow, Pandas, NumPy, MatplotlibWeb Development: Flask, FastAPI, HTML, CSSTools: Power BI, Excel', 'certifications': 'COURSES & CERTIFICATESJun 2025  Jun 2025\\tMicrosoft Power BI — Infosys SpringboardJun 2025 \\tFinQuest — Department of Finance Studies, University of DelhiJun 2025\\tCentral India Hackathon 2.0 — Unstop', 'achievements': 'ACHIEVEMENTSFinalist, Central India Hackathon 2.0Honours in B.Sc.Winner, Hourly Problem (Sandhnaam 2025)Runner-Up, Overnight Problem (Sandhnaam 2025)Winner, MiniHackathon 2024'},\n",
      "None,\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "insert_resume(id, email, \"Resume.docx\", fjson, fcsv, fexcel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18927031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('687faf83a4033d534d855263'), 'csv': None}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_format_resume(id, email, \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c07e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
